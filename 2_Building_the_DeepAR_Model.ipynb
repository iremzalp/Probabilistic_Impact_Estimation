{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b2e6ae-ec2d-4657-9321-c6e398e4daab",
   "metadata": {},
   "source": [
    "# 2. Building the DeepAR Model: Data Preprocessing, Hyperparameter Search, and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a72b7c",
   "metadata": {},
   "source": [
    "In this script, we go through the steps of building a DeepAR forecasting model. We  introduce several classes which will guide us along proces, with the aim of unlocking the potential of DeepAR for accurate and insightful forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436cda0",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d28f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from helper_functions import Helper\n",
    "from agents import Evaluation_Agent\n",
    "from agents import Preparation_Agent\n",
    "from agents import Activity_Agent, Usage_Agent, Load_Agent, Price_Agent\n",
    "\n",
    "helper = Helper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bd1238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "devices = {0: {'hh': 'hh1', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          1: {'hh': 'hh1', 'dev_name': 'Dishwasher', 'dev': 'dishwasher'},\n",
    "          2: {'hh': 'hh2', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          3: {'hh': 'hh2', 'dev_name': 'Dishwasher', 'dev': 'dishwasher'},\n",
    "          4: {'hh': 'hh3', 'dev_name': 'Tumble Dryer', 'dev': 'tumble_dryer'},\n",
    "          5: {'hh': 'hh3', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          6: {'hh': 'hh3', 'dev_name': 'Dishwasher', 'dev': 'dishwasher'},\n",
    "          7: {'hh': 'hh4', 'dev_name': 'Washing Machine (1)', 'dev': 'washing_machine'},\n",
    "          8: {'hh': 'hh4', 'dev_name': 'Washing Machine (2)', 'dev': 'washing_machine'},\n",
    "          9: {'hh': 'hh5', 'dev_name': 'Tumble Dryer', 'dev': 'tumble_dryer'},\n",
    "          10: {'hh': 'hh6', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          11: {'hh': 'hh6', 'dev_name': 'Dishwasher', 'dev': 'dishwasher'},\n",
    "          12: {'hh': 'hh7', 'dev_name': 'Tumble Dryer', 'dev': 'tumble_dryer'},\n",
    "          13: {'hh': 'hh7', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          14: {'hh': 'hh7', 'dev_name': 'Dishwasher', 'dev': 'dishwasher'},\n",
    "          15: {'hh': 'hh8', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          16: {'hh': 'hh9', 'dev_name': 'Washer Dryer', 'dev': 'washer_dryer'},\n",
    "          17: {'hh': 'hh9', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          18: {'hh': 'hh9', 'dev_name': 'Dishwasher', 'dev': 'dishwasher'},\n",
    "          19: {'hh': 'hh10', 'dev_name': 'Washing Machine', 'dev': 'washing_machine'},\n",
    "          }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e808d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hh1': ['Washing Machine', 'Dishwasher'],\n",
       " 'hh2': ['Washing Machine', 'Dishwasher'],\n",
       " 'hh3': ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n",
       " 'hh4': ['Washing Machine (1)', 'Washing Machine (2)'],\n",
       " 'hh5': ['Tumble Dryer'],\n",
       " 'hh6': ['Washing Machine', 'Dishwasher'],\n",
       " 'hh7': ['Tumble Dryer', 'Washing Machine', 'Dishwasher'],\n",
       " 'hh8': ['Washing Machine'],\n",
       " 'hh9': ['Washer Dryer', 'Washing Machine', 'Dishwasher'],\n",
       " 'hh10': ['Washing Machine']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_functions_thesis import Helper_Functions_Thesis\n",
    "shiftable_devices_dict = Helper_Functions_Thesis.create_shiftable_devices_dict(devices)\n",
    "\n",
    "shiftable_devices_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c441a80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hh1', 'hh2', 'hh3', 'hh4', 'hh5', 'hh6', 'hh7', 'hh8', 'hh9', 'hh10'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_appliences_dict_complete = {\n",
    "    'hh1' : ['Washing Machine', 'Dishwasher', 'Computer Site', 'Television Site', 'Electric Heater'], #'Tumble Dryer', \n",
    "    'hh2' : ['Washing Machine', 'Dishwasher', 'Television', 'Microwave', 'Toaster', 'Hi-Fi', 'Kettle', 'Oven Extractor Fan'],\n",
    "    'hh3' : ['Tumble Dryer', 'Washing Machine', 'Dishwasher','Toaster', 'Television', 'Microwave', 'Kettle'],\n",
    "    'hh4' : ['Washing Machine (1)', 'Washing Machine (2)', 'Computer Site', 'Television Site', 'Microwave', 'Kettle'],\n",
    "    'hh5' : ['Tumble Dryer', 'Computer Site', 'Television Site', 'Combination Microwave', 'Kettle', 'Toaster'], # , 'Washing Machine' --> consumes energy constantly; , 'Dishwasher' --> noise at 3am\n",
    "    'hh6' : ['Washing Machine', 'Dishwasher', 'MJY Computer', 'Television Site', 'Microwave', 'Kettle', 'Toaster', 'PGM Computer'],\n",
    "    'hh7' : ['Tumble Dryer', 'Washing Machine', 'Dishwasher', 'Television Site', 'Toaster', 'Kettle'],\n",
    "    'hh8' : ['Washing Machine', 'Toaster', 'Computer', 'Television Site', 'Microwave', 'Kettle'], # 'Dryer' --> consumes constantly\n",
    "    'hh9' : ['Washer Dryer', 'Washing Machine', 'Dishwasher', 'Television Site', 'Microwave', 'Kettle', 'Hi-Fi', 'Electric Heater'], \n",
    "    'hh10' : ['Washing Machine', 'Magimix (Blender)','Television Site', 'Microwave', ' Kenwood KMix'] #'Dishwasher'\n",
    "}\n",
    "\n",
    "active_appliences_dict = {key: active_appliences_dict_complete[key] for key in shiftable_devices_dict}\n",
    "active_appliences_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273e3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_PATH = os.getcwd()+'/data/'\n",
    "PICKLE_PATH = os.getcwd()+'/data/pickle_files'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7528147",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "shiftable_devices = ['Tumble Dryer', 'Washing Machine', 'Dishwasher'] \n",
    "active_appliances = ['Toaster', 'Tumble Dryer', 'Dishwasher', 'Washing Machine','Television', 'Microwave', 'Kettle']\n",
    "\n",
    "#load_parameters\n",
    "truncation_params = {\n",
    "    'features': 'all', \n",
    "    'factor': 1.5, \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "scale_params = {\n",
    "    'features': 'all', \n",
    "    'kind': 'MinMax', \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "aggregate_params = {\n",
    "    'resample_param': '60T'\n",
    "}\n",
    "\n",
    "device_params = {\n",
    "    'threshold': 0.15\n",
    "}\n",
    "\n",
    "load_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate': aggregate_params,\n",
    "    'shiftable_devices': shiftable_devices, \n",
    "    'device': device_params\n",
    "}\n",
    "\n",
    "#activity_parameters\n",
    "\n",
    "activity_params = {\n",
    "    'active_appliances': active_appliances,\n",
    "    'threshold': threshold \n",
    "}\n",
    "\n",
    "time_params = {\n",
    "    'features': ['hour', 'day_name']\n",
    "}\n",
    "\n",
    "activity_lag_params = {\n",
    "    'features': ['activity'],\n",
    "    'lags': [24, 48, 72]\n",
    "}\n",
    "\n",
    "activity_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate': aggregate_params,\n",
    "    'activity': activity_params,\n",
    "    'time': time_params,\n",
    "    'activity_lag': activity_lag_params\n",
    "}\n",
    "\n",
    "#usage_parameters\n",
    "\n",
    "device = {\n",
    "    'threshold' : threshold}\n",
    "\n",
    "aggregate_params24_H = {\n",
    "    'resample_param': '24H'\n",
    "}\n",
    "\n",
    "usage_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'activity': activity_params,\n",
    "    'aggregate_hour': aggregate_params,\n",
    "    'aggregate_day': aggregate_params24_H,\n",
    "    'time': time_params,\n",
    "    'activity_lag': activity_lag_params,\n",
    "    'shiftable_devices' : shiftable_devices,\n",
    "    'device': device\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a84e9",
   "metadata": {},
   "source": [
    "We begin the script by generating outputs from the Usage Agent, Availability Agent, and Load Agent, which contain variables that we will use to train the forecasting model. To expedite the process, pre-generated outputs for all households can be uploaded from the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be954a9-e367-42cc-973e-475a3a81de0f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "activity_dict = {}\n",
    "load_dict = {}\n",
    "usage_dict = {}  \n",
    "\n",
    "for hh in shiftable_devices_dict.keys():\n",
    "    print(f\"Processing household {hh}...\")\n",
    "    \n",
    "    load_pipe_params['shiftable_devices'] = shiftable_devices_dict[hh]\n",
    "    usage_pipe_params['shiftable_devices'] = shiftable_devices_dict[hh]\n",
    "    activity_params['active_appliances'] = active_appliences_dict[hh]\n",
    "    \n",
    "    household_dict = helper.load_household(DATA_PATH, int(hh[2:]), weather_sel=True)\n",
    "    \n",
    "    # calling the preparation agent\n",
    "    prep_dict = Preparation_Agent(household_dict)\n",
    "    activity_dict[hh] = prep_dict.pipeline_activity(household_dict, activity_pipe_params)\n",
    "    load_dict[hh], _, _ = prep_dict.pipeline_load(household_dict, load_pipe_params)\n",
    "    usage_dict[hh] = prep_dict.pipeline_usage(household_dict, usage_pipe_params)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e5f4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dict = Helper_Functions_Thesis.open_pickle_file(PICKLE_PATH, 'load_dict.pickle')\n",
    "activity_dict = Helper_Functions_Thesis.open_pickle_file(PICKLE_PATH, 'activity_dict.pickle')\n",
    "usage_dict = Helper_Functions_Thesis.open_pickle_file(PICKLE_PATH, 'usage_dict.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0021a-0bbe-485b-ac32-e0d8dafee72b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf915e5c",
   "metadata": {},
   "source": [
    "We build the class **Create_Dataset** to construct datasets for each device with all the necessary information to train the forecasting model. We use three agents from the smart home system (load, usage and activity), to gather essential features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f9df5-87be-4efd-8575-624f93c133ed",
   "metadata": {},
   "source": [
    "### 2.1 Initialize the Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dea8be-bd5a-4ba1-a2f5-b3a895d23495",
   "metadata": {},
   "source": [
    "The first step is to initialize the **Create_Dataset** class with the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0ba57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = '2014-03-01'\n",
    "end_dataset = '2015-05-07'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61c38b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Create_Dataset:\n",
    "    '''\n",
    "    A class for constructing datasets for each device with all the necessary information to train\n",
    "    the forecasting model.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_dataset:\n",
    "        A string ('yyyy-mm-dd') to determine the start of the dataset.\n",
    "    end_dataset:\n",
    "        A string ('yyyy-mm-dd') to determine the end of the dataset.\n",
    "    devices:\n",
    "        A dictionary containing device-specific information.\n",
    "    load_dict:\n",
    "        A dictionary containing the output dataframes from the Load Agent for each household.\n",
    "    usage_dict:\n",
    "        A dictionary containing the output dataframes from the Usage Agent for each household.\n",
    "    activity_dict:\n",
    "        A dictionary containing the output dataframes from the Activity Agent for each household.\n",
    "    fill_na:\n",
    "        Bool, controls if missing values should be filled with zeros.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_dataset: str,\n",
    "        end_dataset: str,\n",
    "        devices: Dict[str, Any],\n",
    "        load_dict: Dict[str, pd.DataFrame],\n",
    "        usage_dict: Dict[str, pd.DataFrame],\n",
    "        activity_dict: Dict[str, pd.DataFrame],\n",
    "        fill_na = True,\n",
    "    ):\n",
    "        from helper_functions_thesis import Helper_Functions_Thesis\n",
    "        self.start_dataset = start_dataset\n",
    "        self.end_dataset = end_dataset\n",
    "        self.devices = devices\n",
    "        self.load_dict = load_dict\n",
    "        self.usage_dict = usage_dict\n",
    "        self.activity_dict = activity_dict\n",
    "        self.hh_list: List[str] = list(Helper_Functions_Thesis.create_shiftable_devices_dict(self.devices).keys())\n",
    "        self.fill_na = fill_na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a729a2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_dataset = Create_Dataset(start_dataset, end_dataset, devices, load_dict, usage_dict, activity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7048915-d322-4738-b079-bff1be7c7892",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Slice the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c773bbf",
   "metadata": {},
   "source": [
    "The **slice_datasets** method truncates the load, usage, and activity datasets for each household to the specified time interval defined by start_dataset and end_dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9b76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_datasets(self) -> None:\n",
    "    import pandas as pd\n",
    "\n",
    "    for hh in self.hh_list:\n",
    "        self.load_dict[hh] = self.load_dict[hh].loc[self.start_dataset:self.end_dataset, :]\n",
    "        self.usage_dict[hh] = self.usage_dict[hh].loc[self.start_dataset:self.end_dataset, :]\n",
    "        self.activity_dict[hh] = self.activity_dict[hh].loc[self.start_dataset:self.end_dataset, :]\n",
    "        \n",
    "setattr(Create_Dataset, 'slice_datasets', slice_datasets)\n",
    "del slice_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset.slice_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2d5b9",
   "metadata": {},
   "source": [
    "### 2.3 Fill NA Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b45227",
   "metadata": {},
   "source": [
    "The **'fill_na_function'** method replaces NA values of the Agent outputs with zeros. The method is run if the *'fill_na'* variable is set to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52fd05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_function(self) -> None:\n",
    "    import pandas as pd\n",
    "\n",
    "    for hh in self.hh_list:\n",
    "        self.load_dict[hh] = self.load_dict[hh].fillna(0)\n",
    "        self.usage_dict[hh] = self.usage_dict[hh].fillna(0)\n",
    "        self.activity_dict[hh] = self.activity_dict[hh].fillna(0)\n",
    "        \n",
    "setattr(Create_Dataset, 'fill_na_function', fill_na_function)\n",
    "del fill_na_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b8c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_dataset.fill_na:\n",
    "    create_dataset.fill_na_function()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e508461",
   "metadata": {},
   "source": [
    "### 2.4 Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d79d2c",
   "metadata": {},
   "source": [
    "We will train the forecasting model with the following variables:\n",
    "1. periods_since_last_activity\n",
    "2. periods_since_last_usage\n",
    "3. activity_prob\n",
    "4. usage_prob\n",
    "5. temp (temparature)\n",
    "6. dwpt (dew point)\n",
    "7. rhum (humidity)\n",
    "8. wdir (wind direction)\n",
    "9. wspd (wind speed)\n",
    "10. hh (household id, static variable)\n",
    "11. dev (type of device, static variable)\n",
    "\n",
    "As one might notice, there are no time features explicitly present in the list of variables, despite them having considerable influence on any forecasting task. This is attributed to the capabilitiy of the DeepAR to autonomously generate and integrate time-related features, capturing temporal dependencies without manual intervention. This property of DeepAR allows the model to learn various temporal patterns and enhances prediction power while simplifying the dataset preparation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846f30a-a2b5-40b0-ad5b-a7fa57b327c6",
   "metadata": {},
   "source": [
    "#### 2.4.1 Compute activity and usage probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ff5c1-9bdd-4290-887a-928859f36e52",
   "metadata": {},
   "source": [
    "First step is to create the usage and activity probability variables using the corresponding agents from the recommendation system. These variables offer a more nuanced perspective beyond historical usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce0b861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_activity_probability(self, activity_dict: pd.DataFrame) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    from datetime import timedelta\n",
    "    from agents import Activity_Agent\n",
    "\n",
    "    activity = Activity_Agent(activity_dict)\n",
    "    X_train, y_train, X_test, y_test = activity.train_test_split(\n",
    "        activity_dict, str(activity_dict.index[-1].date() + timedelta(1))\n",
    "    )\n",
    "    X_train = X_train.fillna(0)\n",
    "    model = activity.fit(X_train, y_train, 'random forest')\n",
    "    activity_probability = model.predict_proba(X_train)[:, 1]\n",
    "    return activity_probability\n",
    "\n",
    "setattr(Create_Dataset, 'create_activity_probability', create_activity_probability)\n",
    "del create_activity_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb75481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_usage_probability(self, usage_dict: pd.DataFrame, dev_name: str) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    from agents import Usage_Agent\n",
    "\n",
    "    usage = Usage_Agent(usage_dict, dev_name)\n",
    "    X_train, y_train, X_test, y_test = usage.train_test_split(\n",
    "        usage_dict, self.end_dataset, train_start=self.start_dataset\n",
    "    )\n",
    "    X_train = X_train.fillna(0) \n",
    "    model = usage.fit(X_train, y_train.values.ravel(), 'random forest')\n",
    "    usage_probability = model.predict_proba(X_train)[:, 1]\n",
    "    return usage_probability\n",
    "\n",
    "setattr(Create_Dataset, 'create_usage_probability', create_usage_probability)\n",
    "del create_usage_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559e1b1",
   "metadata": {},
   "source": [
    "#### 2.4.2 Convert Daily Data to Hourly Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdcd5a",
   "metadata": {},
   "source": [
    "There are three variables (periods_since_last_activity, periods_since_last_usage, usage_prob) which capture daily trends in device usage dynamics. To align these variables with the hourly granularity required for forecasting, we employ the **resample_daily_to_hourly_data** function. This method transforms the daily dataframes into hourly resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3441981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_daily_to_hourly_data(self, daily_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    import pandas as pd\n",
    "    from agents import Activity_Agent\n",
    "\n",
    "    dummy_day = daily_data.index[-1] + pd.Timedelta(days=1)\n",
    "    dummy_row = pd.DataFrame(index=pd.DatetimeIndex([dummy_day]))\n",
    "    daily_data_with_dummy = pd.concat([daily_data, dummy_row])\n",
    "    hourly_data = daily_data_with_dummy.resample(\"H\").ffill().iloc[:-1, :]\n",
    "    return hourly_data\n",
    "\n",
    "setattr(Create_Dataset, 'resample_daily_to_hourly_data', resample_daily_to_hourly_data)\n",
    "del resample_daily_to_hourly_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc89a6-ab7b-4af9-a305-e23c863c72a9",
   "metadata": {},
   "source": [
    "#### 2.4.3 Create Feature Datasets for Each Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c9ed3",
   "metadata": {},
   "source": [
    "Now that we have all the necessary functions at hand to generate the necessary variables to train the DeepAR model, we can structure the data into separate dataframes for each shiftable device within the household with the **'create_deepar_dataset'** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "510bae1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_deepar_dataset(self) -> Dict[str, pd.DataFrame]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    deepar_dataset = {}\n",
    "\n",
    "    for dev in self.devices.keys():\n",
    "        hh = self.devices[dev]['hh']\n",
    "        dev_name = self.devices[dev]['dev_name']\n",
    "\n",
    "        # Create a DataFrame with required columns\n",
    "        deepar_dataset[dev] = pd.DataFrame({\n",
    "            'temp': self.activity_dict[hh]['temp'],\n",
    "            'dwpt': self.activity_dict[hh]['dwpt'],\n",
    "            'rhum': self.activity_dict[hh]['rhum'],\n",
    "            'wdir': self.activity_dict[hh]['wdir'],\n",
    "            'wspd': self.activity_dict[hh]['wspd'],\n",
    "            'usage': self.load_dict[hh][dev_name],\n",
    "            'usage_bin': np.where(self.load_dict[hh][dev_name] > 0, 1, 0),\n",
    "            'hh': hh,\n",
    "            'dev': self.devices[dev]['dev'],\n",
    "            'activity_prob': self.create_activity_probability(self.activity_dict[hh])\n",
    "        })\n",
    "\n",
    "        # add usage-related features\n",
    "        columns = ['periods_since_last_activity', f'periods_since_last_{dev_name}_usage']\n",
    "        daily_features = self.usage_dict[hh][columns].copy()\n",
    "        daily_features.columns = daily_features.columns.str.replace(f'{dev_name}_', '', regex=False)\n",
    "        daily_features['usage_prob'] = self.create_usage_probability(self.usage_dict[hh].copy(), dev_name)\n",
    "\n",
    "        # Resample daily data to hourly and concatenate\n",
    "        hourly_features = self.resample_daily_to_hourly_data(daily_features)\n",
    "        deepar_dataset[dev] = pd.concat([deepar_dataset[dev], hourly_features], axis=1)\n",
    "\n",
    "        # Select final columns\n",
    "        columns = [\n",
    "            'usage', 'usage_bin', 'hh', 'dev',\n",
    "            'periods_since_last_activity', 'periods_since_last_usage',\n",
    "            'activity_prob', 'usage_prob',\n",
    "            'temp', 'dwpt', 'rhum', 'wdir', 'wspd'\n",
    "        ]\n",
    "\n",
    "        deepar_dataset[dev] = deepar_dataset[dev][columns]\n",
    "\n",
    "    return deepar_dataset\n",
    "\n",
    "setattr(Create_Dataset, 'create_deepar_dataset', create_deepar_dataset)\n",
    "del create_deepar_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678cb977-8747-4d45-9f82-d876f3c98622",
   "metadata": {},
   "source": [
    "### 2.5 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "918e01f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline(self) -> Dict[str, pd.DataFrame]:\n",
    "    \n",
    "    self.slice_datasets()\n",
    "    \n",
    "    if self.fill_na:\n",
    "            self.fill_na_function()\n",
    "\n",
    "    deepar_dataset = self.create_deepar_dataset()\n",
    "\n",
    "    return deepar_dataset\n",
    "\n",
    "setattr(Create_Dataset, 'pipeline', pipeline)\n",
    "del pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bbba00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usage</th>\n",
       "      <th>usage_bin</th>\n",
       "      <th>hh</th>\n",
       "      <th>dev</th>\n",
       "      <th>periods_since_last_activity</th>\n",
       "      <th>periods_since_last_usage</th>\n",
       "      <th>activity_prob</th>\n",
       "      <th>usage_prob</th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-03-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>hh1</td>\n",
       "      <td>washing_machine</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.609412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-01 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>hh1</td>\n",
       "      <td>washing_machine</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.609412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-01 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>hh1</td>\n",
       "      <td>washing_machine</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.609412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     usage  usage_bin   hh              dev  \\\n",
       "2014-03-01 00:00:00    0.0          0  hh1  washing_machine   \n",
       "2014-03-01 01:00:00    0.0          0  hh1  washing_machine   \n",
       "2014-03-01 02:00:00    0.0          0  hh1  washing_machine   \n",
       "\n",
       "                     periods_since_last_activity  periods_since_last_usage  \\\n",
       "2014-03-01 00:00:00                         36.0                      37.0   \n",
       "2014-03-01 01:00:00                         36.0                      37.0   \n",
       "2014-03-01 02:00:00                         36.0                      37.0   \n",
       "\n",
       "                     activity_prob  usage_prob      temp      dwpt      rhum  \\\n",
       "2014-03-01 00:00:00       0.006815    0.004902  0.819444  0.518519  0.548148   \n",
       "2014-03-01 01:00:00       0.007854    0.004902  0.819444  0.518519  0.548148   \n",
       "2014-03-01 02:00:00       0.005902    0.004902  0.819444  0.518519  0.548148   \n",
       "\n",
       "                         wdir      wspd  \n",
       "2014-03-01 00:00:00  0.571429  0.609412  \n",
       "2014-03-01 01:00:00  0.571429  0.609412  \n",
       "2014-03-01 02:00:00  0.571429  0.609412  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = create_dataset.pipeline()\n",
    "dataset_dict[0].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb0d25",
   "metadata": {},
   "source": [
    "## 3. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb7e97",
   "metadata": {},
   "source": [
    "After preparing the data and shaping it into individual dataframes, our attention turns towards transforming them into a format compatible with GluonTS, a powerful time series forecasting library that includes DeepAR as one of its models. This transformation involves partitioning the data into test, validation, and training sets. We will create these datasets in ListDataset format used by Gluonts, where each time series is represented as a dictionary containing the target values and related features.\n",
    "\n",
    "The structure of the **'Transform_Dataset'** class is inspired from the following GluonTS template: https://www.kaggle.com/code/steverab/m5-forecasting-competition-gluonts-template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd95705-7113-4046-a159-f9a883a8b02f",
   "metadata": {},
   "source": [
    "### 3.1 Initialize the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c200b",
   "metadata": {},
   "source": [
    "In a first step, we initialize the **'Transform_Dataset'** class with the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a154f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_validation = '2015-01-01'\n",
    "freq = '1H'\n",
    "prediction_length = 24 #in freq granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1549056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transform_Dataset:\n",
    "    '''\n",
    "    A class for reshaping datasets into a format compatible with GluonTS, the forecasting model.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_training:\n",
    "        A string ('yyyy-mm-dd') determining the start of the training period.\n",
    "    start_validation:\n",
    "        A string ('yyyy-mm-dd') determining the start of the testing period.\n",
    "    devices:\n",
    "        A dictionary containing device-specific information.\n",
    "    dataset_dict:\n",
    "        A dictionary with target time series data along with covariates.\n",
    "    freq:\n",
    "        A string specifying the frequency of the time series data (e.g., '1H' for hourly data).\n",
    "    prediction_length:\n",
    "        An integer representing the desired prediction horizon (default is 24).\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_training: str,\n",
    "        start_validation: str,\n",
    "        devices: Dict[str, Any],\n",
    "        dataset_dict: Dict[str, pd.DataFrame],\n",
    "        freq: str = '1H',\n",
    "        prediction_length: int = 24,\n",
    "    ):\n",
    "        self.start_training = str(start_training)\n",
    "        self.start_validation = str(start_validation)\n",
    "        self.hh_list: List[str] = []\n",
    "        self.devices = devices\n",
    "        self.dataset_dict = {hh: df.copy() for hh, df in dataset_dict.items()}\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca69873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = Transform_Dataset(start_dataset, start_validation, devices, dataset_dict, freq, prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6de936-3313-45c2-b142-d77ab79acf19",
   "metadata": {},
   "source": [
    "### 3.2 Reshape the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddddfa",
   "metadata": {},
   "source": [
    "The **'reshape_training_data'** method creates a single dataframe where each row represents a device and the columns represent the usage history. Furthermore we add three more columns with the relevant static variables, namely the household id, device id, and the devices' unique identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff293e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_training_data(self):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    transposed_list = {}\n",
    "    \n",
    "    for dev in self.devices.keys():\n",
    "        \n",
    "        self.dataset_dict[dev] = self.dataset_dict[dev].loc[:self.start_validation]\n",
    "        usage_bin_df = self.dataset_dict[dev]['usage_bin']\n",
    "        usage_bin_df = usage_bin_df.reindex()\n",
    "        usage_bin_df = pd.DataFrame(usage_bin_df)\n",
    "        usage_bin_df.index = ['d' + str(n) for n in np.arange(usage_bin_df.shape[0])]\n",
    "        usage_bin_df = pd.DataFrame(usage_bin_df).T\n",
    "\n",
    "        usage_bin_df['dev'] = self.devices[dev]['dev']\n",
    "        usage_bin_df['hh'] = str(self.devices[dev]['hh'])\n",
    "        usage_bin_df['id'] = usage_bin_df['hh'] + '_' + usage_bin_df['dev']\n",
    "        transposed_list[dev] = usage_bin_df\n",
    "\n",
    "    training_dataset = pd.concat(transposed_list.values()).reset_index(drop=True)\n",
    "\n",
    "    return training_dataset\n",
    "\n",
    "setattr(Transform_Dataset, 'reshape_training_data', reshape_training_data)\n",
    "del reshape_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a44948d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d0</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>...</th>\n",
       "      <th>d7361</th>\n",
       "      <th>d7362</th>\n",
       "      <th>d7363</th>\n",
       "      <th>d7364</th>\n",
       "      <th>d7365</th>\n",
       "      <th>d7366</th>\n",
       "      <th>d7367</th>\n",
       "      <th>dev</th>\n",
       "      <th>hh</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>washing_machine</td>\n",
       "      <td>hh1</td>\n",
       "      <td>hh1_washing_machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dishwasher</td>\n",
       "      <td>hh1</td>\n",
       "      <td>hh1_dishwasher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>washing_machine</td>\n",
       "      <td>hh2</td>\n",
       "      <td>hh2_washing_machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 7371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   d0  d1  d2  d3  d4  d5  d6  d7  d8  d9  ...  d7361  d7362  d7363  d7364  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...      0      0      0      0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...      0      0      0      0   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...      0      0      0      0   \n",
       "\n",
       "   d7365  d7366  d7367              dev   hh                   id  \n",
       "0      0      0      0  washing_machine  hh1  hh1_washing_machine  \n",
       "1      0      0      0       dishwasher  hh1       hh1_dishwasher  \n",
       "2      0      0      0  washing_machine  hh2  hh2_washing_machine  \n",
       "\n",
       "[3 rows x 7371 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = transformer.reshape_training_data()\n",
    "training_dataset.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae8d09-d3a1-4013-a904-0327de6aee32",
   "metadata": {},
   "source": [
    "### 3.3 Create dynamic features list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00006a1e",
   "metadata": {},
   "source": [
    "Next, we generate dynamic real features for training, validation, and testing using the **'create_dynamic_real_features'** method. These features encompass time-varying continuous variables that describe the activity and usage patterns, as well as the weather conditions. DeepAR leverages these dynamic contextual factors to better capture the underlying patterns within the data.\n",
    "\n",
    "The dynamic variables are extracted and transposed to a numpy array. For the training and validation lists, we crop the input sequences by removing the last time steps (of prediction lenght) to create a clear separation between the training data and the prediction horizon. This guarantees that the model does not have access to the actual target values it needs to predict during training and validation, thereby preventing data leakage and ensuring that the model is evaluated on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "707f7fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dynamic_real_features(self):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    train_dynamic_real_features_list = []\n",
    "    val_dynamic_real_features_list = []\n",
    "    test_dynamic_real_features_list = []\n",
    "\n",
    "    for dev in self.devices.keys():\n",
    "        dynamic_real_columns = self.dataset_dict[dev].drop(columns=['usage','usage_bin','hh','dev']).columns\n",
    "\n",
    "        train_dynamic_real_features = (\n",
    "            self.dataset_dict[dev]\n",
    "            .iloc[: -self.prediction_length * 2, :][dynamic_real_columns]\n",
    "            .T.to_numpy()\n",
    "        )\n",
    "        val_dynamic_real_features = (\n",
    "            self.dataset_dict[dev]\n",
    "            .iloc[:-self.prediction_length, :][dynamic_real_columns]\n",
    "            .T.to_numpy()\n",
    "        )\n",
    "        test_dynamic_real_features = self.dataset_dict[dev][\n",
    "            dynamic_real_columns\n",
    "        ].T.to_numpy()\n",
    "        \n",
    "        train_dynamic_real_features_list.append(train_dynamic_real_features)\n",
    "        val_dynamic_real_features_list.append(val_dynamic_real_features)\n",
    "        test_dynamic_real_features_list.append(test_dynamic_real_features)\n",
    "\n",
    "    return (\n",
    "        train_dynamic_real_features_list,\n",
    "        val_dynamic_real_features_list,\n",
    "        test_dynamic_real_features_list,\n",
    "    )\n",
    "\n",
    "setattr(Transform_Dataset, 'create_dynamic_real_features', create_dynamic_real_features)\n",
    "del create_dynamic_real_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bc52cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 7320)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_dynamic_real_features_list,\n",
    " val_dynamic_real_features_list,\n",
    " test_dynamic_real_features_list,\n",
    "    ) = transformer.create_dynamic_real_features()\n",
    "print(len(train_dynamic_real_features_list))\n",
    "train_dynamic_real_features_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a00e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 7320)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_dynamic_real_features_list,\n",
    " val_dynamic_real_features_list,\n",
    " test_dynamic_real_features_list,\n",
    "    ) = transformer.create_dynamic_real_features()\n",
    "print(len(train_dynamic_real_features_list))\n",
    "train_dynamic_real_features_list[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91520d-9cc3-4101-9f1e-3a59dd10f974",
   "metadata": {},
   "source": [
    "### 3.4 Create static features list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f7a7d",
   "metadata": {},
   "source": [
    "Next, we define the **'create_static_categorical_features'** function to generate static categorical variables. The function outputs for each device the unique household id and the device type. These values remain constant throughout the time series, and don't need to be repeated along each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd6c094b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_static_categorical_features(self, training_dataset: pd.DataFrame) -> Tuple[Any, Tuple[int, int]]:\n",
    "    import numpy as np\n",
    "    \n",
    "    hh_ids = training_dataset['hh'].astype('category').cat.codes.values\n",
    "    hh_ids_unique = np.unique(hh_ids)\n",
    "\n",
    "    dev_ids = training_dataset['dev'].astype('category').cat.codes.values\n",
    "    dev_ids_unique = np.unique(dev_ids)\n",
    "\n",
    "    stat_cat_list = [hh_ids, dev_ids]\n",
    "\n",
    "    stat_cat = np.concatenate(stat_cat_list)\n",
    "    stat_cat = stat_cat.reshape(len(stat_cat_list), len(dev_ids)).T\n",
    "        \n",
    "    stat_cat_cardinalities: Tuple[int, int] = (len(hh_ids_unique), len(dev_ids_unique))\n",
    "        \n",
    "    return stat_cat, stat_cat_cardinalities\n",
    "\n",
    "setattr(Transform_Dataset, 'create_static_categorical_features', create_static_categorical_features)\n",
    "del create_static_categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "894dbcab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat_cat_cardinalities: (10, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 3],\n",
       "       [0, 0],\n",
       "       [2, 3],\n",
       "       [2, 0],\n",
       "       [3, 1],\n",
       "       [3, 3],\n",
       "       [3, 0],\n",
       "       [4, 3],\n",
       "       [4, 3],\n",
       "       [5, 1],\n",
       "       [6, 3],\n",
       "       [6, 0],\n",
       "       [7, 1],\n",
       "       [7, 3],\n",
       "       [7, 0],\n",
       "       [8, 3],\n",
       "       [9, 2],\n",
       "       [9, 3],\n",
       "       [9, 0],\n",
       "       [1, 3]], dtype=int8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_cat_cardinalities = transformer.create_static_categorical_features(training_dataset)[1]\n",
    "stat_cat = transformer.create_static_categorical_features(training_dataset)[0]\n",
    "\n",
    "print('stat_cat_cardinalities:', stat_cat_cardinalities)\n",
    "stat_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0435867-d3cd-49b2-8f25-0569f1f045fd",
   "metadata": {},
   "source": [
    "### 3.5 Split the data into training, validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdcb86f",
   "metadata": {},
   "source": [
    "Finally we split the preprocessed data along the covariates into training, validation, and test datasets using the GluonTS library. From the training_dataset we extract the target values for each device usage and create lists of target values for training, validation and testing by cropping them according to the prediction length. Additionally, we create the date list based on the specified start time. Finally, we construct the ListDataset objects to store the time series data along with associated features.\n",
    "\n",
    "After running this function, we get for each device to be predicted a dictionary containing the target values, the start time of the time series, the dynamic real features, and the static categorical features. This format aligns with the DeepAR model's requirements for effective training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "308dfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    self,\n",
    "    training_dataset: pd.DataFrame,\n",
    "    train_dynamic_real_features_list: List[Any],\n",
    "    val_dynamic_real_features_list: List[Any],\n",
    "    test_dynamic_real_features_list: List[Any],\n",
    "    stat_cat: Any\n",
    ") -> Tuple[Any, Any, Any]:\n",
    "    import pandas as pd\n",
    "    from gluonts.dataset.common import ListDataset\n",
    "    from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "    train_df = training_dataset.drop(['id', 'hh', 'dev'], axis=1)\n",
    "    train_target_values = train_df.values\n",
    "\n",
    "    test_target_values = val_target_values = train_target_values.copy()\n",
    "\n",
    "    train_target_values = [ts[:-self.prediction_length * 2] for ts in train_target_values]\n",
    "    val_target_values = [ts[:-self.prediction_length] for ts in val_target_values]\n",
    "\n",
    "    dates = [pd.Timestamp(self.start_training) for _ in range(len(training_dataset))]\n",
    "\n",
    "    def create_list_dataset(target_values, dynamic_real_features_list):\n",
    "        return ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.TARGET: target,\n",
    "                    FieldName.START: start,\n",
    "                    FieldName.FEAT_DYNAMIC_REAL: fdr,\n",
    "                    FieldName.FEAT_STATIC_CAT: fsc,\n",
    "                }\n",
    "                for (target, start, fdr, fsc) in zip(\n",
    "                    target_values,\n",
    "                    dates,\n",
    "                    dynamic_real_features_list,\n",
    "                    stat_cat,\n",
    "                )\n",
    "            ],\n",
    "            freq=self.freq,\n",
    "        )\n",
    "\n",
    "    train_ds = create_list_dataset(train_target_values, train_dynamic_real_features_list)\n",
    "    val_ds = create_list_dataset(val_target_values, val_dynamic_real_features_list)\n",
    "    test_ds = create_list_dataset(test_target_values, test_dynamic_real_features_list)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "setattr(Transform_Dataset, 'split_data', split_data)\n",
    "del split_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f266996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 7320)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, _, _, = transformer.split_data(training_dataset,\n",
    "                                          train_dynamic_real_features_list,\n",
    "                                          val_dynamic_real_features_list,\n",
    "                                          test_dynamic_real_features_list,\n",
    "                                          stat_cat\n",
    "                                         )\n",
    "train_ds[0]['feat_dynamic_real'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb662f-70eb-4f25-8150-0956934620a8",
   "metadata": {},
   "source": [
    "### 3.6 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156dfd14-b5aa-4667-9bb0-2072fde8025a",
   "metadata": {},
   "source": [
    "Finally, the pipeline function streamlines the process and generates training, validation, and test datasets for training and evaluating the DeepAR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fda70365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline(self) -> Tuple[Any, Any, Any]:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from gluonts.dataset.common import ListDataset\n",
    "    from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "    training_dataset = self.reshape_training_data()\n",
    "\n",
    "    (\n",
    "        train_dynamic_real_features_list,\n",
    "        val_dynamic_real_features_list,\n",
    "        test_dynamic_real_features_list,\n",
    "    ) = self.create_dynamic_real_features()\n",
    "        \n",
    "    stat_cat = self.create_static_categorical_features(training_dataset)[0]\n",
    "\n",
    "    train_ds, val_ds, test_ds = self.split_data(\n",
    "        training_dataset,\n",
    "        train_dynamic_real_features_list,\n",
    "        val_dynamic_real_features_list,\n",
    "        test_dynamic_real_features_list,\n",
    "        stat_cat\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "setattr(Transform_Dataset, 'pipeline', pipeline)\n",
    "del pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a02e4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " 'start': Period('2014-03-01 00:00', 'H'),\n",
       " 'feat_dynamic_real': array([[3.6000000e+01, 3.6000000e+01, 3.6000000e+01, ..., 1.0000000e+00,\n",
       "         1.0000000e+00, 1.0000000e+00],\n",
       "        [3.7000000e+01, 3.7000000e+01, 3.7000000e+01, ..., 2.0000000e+00,\n",
       "         2.0000000e+00, 2.0000000e+00],\n",
       "        [6.8152905e-03, 7.8541255e-03, 5.9017446e-03, ..., 9.9433905e-01,\n",
       "         9.8637235e-01, 9.9727309e-01],\n",
       "        ...,\n",
       "        [5.4814816e-01, 5.4814816e-01, 5.4814816e-01, ..., 6.6666669e-01,\n",
       "         6.6666669e-01, 6.6666669e-01],\n",
       "        [5.7142860e-01, 5.7142860e-01, 5.7142860e-01, ..., 5.7142860e-01,\n",
       "         5.7142860e-01, 5.7142860e-01],\n",
       "        [6.0941178e-01, 6.0941178e-01, 6.0941178e-01, ..., 0.0000000e+00,\n",
       "         1.0000000e+00, 9.1529411e-01]], dtype=float32),\n",
       " 'feat_static_cat': array([0, 3], dtype=int32)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, test_ds, = transformer.pipeline()\n",
    "test_ds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b5e3d",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce06a5",
   "metadata": {},
   "source": [
    "With our datasets prepared, we are now ready to initiate the hyperparameter search, aiming to discover the optimal model configuration for the forecasting task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81501f3",
   "metadata": {},
   "source": [
    "### 4.1 Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3d9af",
   "metadata": {},
   "source": [
    "Conducting hyperparameter search for a DeepAR model can be time-sensitive. To efficiently manage this, we implement a custom callback that can terminate the training process early based on a specified metric value. The class **'Metric_Inference_Early_Stopping'** is taken from the GluonTS website, where they introduce callbacks to the Trainer class.\n",
    "\n",
    "Link: https://ts.gluon.ai/dev/tutorials/advanced_topics/trainer_callbacks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9769bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.mx.trainer.callback import Callback\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.dataset.common import Dataset\n",
    "from gluonts.mx import DeepAREstimator # Trainer\n",
    "\n",
    "class Metric_Inference_Early_Stopping(Callback):\n",
    "    '''\n",
    "    Early Stopping mechanism based on the prediction network.\n",
    "    Can be used to base the Early Stopping directly on a metric of interest, instead of on the training/validation loss.\n",
    "    In the same way as test datasets are used during model evaluation,\n",
    "    the time series of the validation_dataset can overlap with the train dataset time series,\n",
    "    except for a prediction_length part at the end of each time series.\n",
    "    Parameters\n",
    "    ----------\n",
    "    validation_dataset\n",
    "        An out-of-sample dataset which is used to monitor metrics\n",
    "    predictor\n",
    "        A gluon predictor, with a prediction network that matches the training network\n",
    "    evaluator\n",
    "        The Evaluator used to calculate the validation metrics.\n",
    "    metric\n",
    "        The metric on which to base the early stopping on.\n",
    "    patience\n",
    "        Number of epochs to train on given the metric did not improve more than min_delta.\n",
    "    min_delta\n",
    "        Minimum change in the monitored metric counting as an improvement\n",
    "    verbose\n",
    "        Controls, if the validation metric is printed after each epoch.\n",
    "    minimize_metric\n",
    "        The metric objective.\n",
    "    restore_best_network\n",
    "        Controls, if the best model, as assessed by the validation metrics is restored after training.\n",
    "    num_samples\n",
    "        The amount of samples drawn to calculate the inference metrics.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        validation_dataset: Dataset,\n",
    "        estimator: DeepAREstimator,\n",
    "        evaluator: Evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9],allow_nan_forecast=True),\n",
    "        metric: str = 'MSE',\n",
    "        patience: int = 20,\n",
    "        min_delta: float = 0.0,\n",
    "        verbose: bool = False,\n",
    "        minimize_metric: bool = True,\n",
    "        restore_best_network: bool = True,\n",
    "        num_samples: int = 100,\n",
    "    ):\n",
    "        assert (\n",
    "            patience >= 0\n",
    "        ), 'EarlyStopping Callback patience needs to be >= 0'\n",
    "        assert (\n",
    "            min_delta >= 0\n",
    "        ), 'EarlyStopping Callback min_delta needs to be >= 0.0'\n",
    "        assert (\n",
    "            num_samples >= 1\n",
    "        ), 'EarlyStopping Callback num_samples needs to be >= 1'\n",
    "\n",
    "        self.validation_dataset = list(validation_dataset)\n",
    "        self.estimator = estimator\n",
    "        self.evaluator = evaluator\n",
    "        self.metric = metric\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.restore_best_network = restore_best_network\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        if minimize_metric:\n",
    "            self.best_metric_value = np.inf\n",
    "            self.is_better = np.less\n",
    "        else:\n",
    "            self.best_metric_value = -np.inf\n",
    "            self.is_better = np.greater\n",
    "\n",
    "        self.validation_metric_history: List[float] = []\n",
    "        self.best_network = None\n",
    "        self.n_stale_epochs = 0\n",
    "\n",
    "    def on_epoch_end(\n",
    "        self,\n",
    "        epoch_no: int,\n",
    "        epoch_loss: float,\n",
    "        training_network: mx.gluon.nn.HybridBlock,\n",
    "        trainer: mx.gluon.Trainer,\n",
    "        best_epoch_info: dict,\n",
    "        ctx: mx.Context\n",
    "    ) -> bool:\n",
    "        should_continue = True\n",
    "        \n",
    "        transformation = self.estimator.create_transformation()\n",
    "        predictor = self.estimator.create_predictor(transformation=transformation, trained_network=training_network)\n",
    "\n",
    "        from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=self.validation_dataset,\n",
    "            predictor=predictor,\n",
    "            num_samples=self.num_samples,\n",
    "        )\n",
    "\n",
    "        agg_metrics, item_metrics = self.evaluator(ts_it, forecast_it)\n",
    "        current_metric_value = agg_metrics[self.metric]\n",
    "        self.validation_metric_history.append(current_metric_value)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation metric {self.metric}: {current_metric_value}, best: {self.best_metric_value}'\n",
    "            )\n",
    "\n",
    "        if self.is_better(current_metric_value, self.best_metric_value):\n",
    "            self.best_metric_value = current_metric_value\n",
    "\n",
    "            if self.restore_best_network:\n",
    "                training_network.save_parameters('best_network.params')\n",
    "\n",
    "            self.n_stale_epochs = 0\n",
    "        else:\n",
    "            self.n_stale_epochs += 1\n",
    "            if self.n_stale_epochs == self.patience:\n",
    "                should_continue = False\n",
    "                print(\n",
    "                    f'EarlyStopping callback initiated stop of training at epoch {epoch_no}.'\n",
    "                )\n",
    "\n",
    "                if self.restore_best_network:\n",
    "                    print(\n",
    "                        f'Restoring best network from epoch {epoch_no - self.patience}.'\n",
    "                    )\n",
    "                    training_network.load_parameters('best_network.params')\n",
    "\n",
    "        return should_continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747baf89",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279edd79",
   "metadata": {},
   "source": [
    "We will tune the hyperparameters of the DeepAR model using Optuna, an automated hyperparameter optimization framework that explores the search space to find optimal configurations. The initial class created for model tuning can be found on the GluonTS website: https://ts.gluon.ai/dev/tutorials/advanced_topics/hp_tuning_with_optuna.html\n",
    "\n",
    "However, we make small changes to this class so that it fits our task of evaluating the model over a longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f062e9",
   "metadata": {},
   "source": [
    "#### 4.2.1 Initialize the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52015d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_length = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "816f3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.mx.distribution import CategoricalOutput\n",
    "from gluonts.mx import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "class DeepAR_Tuning_Objective:\n",
    "    '''\n",
    "    A class for hyperparameter tuning using Optuna.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prediction_length:\n",
    "        An integer representing the desired prediction horizon.\n",
    "    freq:\n",
    "        A string specifying the frequency of the time series data.\n",
    "    start_training:\n",
    "        A string ('yyyy-mm-dd') determining the start of the training period.\n",
    "    start_validation:\n",
    "        A string ('yyyy-mm-dd') determining the start of the validation period.\n",
    "    recommendation_length:\n",
    "        The length of the test period in days.\n",
    "    devices:\n",
    "        A dictionary containing device-specific information.\n",
    "    dataset_dict:\n",
    "        A dictionary with target time series data along with covariates.\n",
    "    metric_type:\n",
    "        A string specifying the type of metric to optimize during tuning (default is 'MASE').\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length: int,\n",
    "        freq: str,\n",
    "        start_training: str,\n",
    "        start_validation: str,\n",
    "        recommendation_length: int,\n",
    "        devices: Dict[str, Any],\n",
    "        dataset_dict: Dict[str, pd.DataFrame],\n",
    "        metric_type='MASE', \n",
    "    ):\n",
    "        self.prediction_length = prediction_length\n",
    "        self.freq = freq\n",
    "        self.start_training = start_training\n",
    "        self.start_validation = start_validation\n",
    "        self.recommendation_length = recommendation_length\n",
    "        self.devices = devices\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.metric_type = metric_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ae0d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepAR_tuning = DeepAR_Tuning_Objective(prediction_length, \n",
    "                                        freq,\n",
    "                                        start_dataset,\n",
    "                                        start_validation,\n",
    "                                        recommendation_length,\n",
    "                                        devices,\n",
    "                                        dataset_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87823f08",
   "metadata": {},
   "source": [
    "#### 4.2.2 Define the Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a52f0",
   "metadata": {},
   "source": [
    "With the **'get_params'** function, we define what hyperparameters to be tuned. These hyperparameters are sampled within specified ranges during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e72864a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(self, trial) -> dict:\n",
    "    return {\n",
    "        # Number of time steps in the context window (3, 4, or 5 weeks)\n",
    "        'context_length': trial.suggest_categorical('context_length', [504, 672, 840]),\n",
    "        \n",
    "        # Number of layers in the network\n",
    "        'num_layers': trial.suggest_int('num_layers', 3, 4),\n",
    "        \n",
    "        # Number of cells in each layer\n",
    "        'num_cells': trial.suggest_int('num_cells', 10, 50),\n",
    "        \n",
    "        # Dropout rate to prevent overfitting\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        \n",
    "        # Learning rate for optimization (log scale)\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        \n",
    "        # Number of training epochs\n",
    "        'epochs': trial.suggest_int('epochs', 25, 100),\n",
    "        \n",
    "        # Batch size for training\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        \n",
    "        # Number of batches per epoch\n",
    "        'num_batches_per_epoch': trial.suggest_categorical('num_batches_per_epoch', [40, 50, 60, 70, 80, 90]),\n",
    "    }\n",
    "\n",
    "setattr(DeepAR_Tuning_Objective, 'get_params', get_params)\n",
    "del get_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4263c",
   "metadata": {},
   "source": [
    "#### 4.2.3 Hyperparameter Tuning and Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e48b6",
   "metadata": {},
   "source": [
    "\n",
    "In the **'\\_call_'** function, we set up the DeepAR model for training and evaluation. Here's a step-by-step breakdown of the process:\n",
    "\n",
    "1. Hyperparameter Setup:\n",
    "We obtain a set of hyperparameters using the **'get_params'** method. Optuna provides suggestions for these hyperparameters during each trial. With these hyperparameters, we configure an instance of the DeepAR model. The estimator is set up with other relevant configuration parameters, like the prediction length, the frequency of the time series, output distribution (CategoricalOutput), and the use of dynamic and static features.\n",
    "\n",
    "2. Evaluator Configuration:\n",
    "We establish an Evaluator function which will help us assess how well the model performs.\n",
    "\n",
    "3. Data Preparation for Training and Evaluation:\n",
    "To train and evaluate the model, we begin by creating a date list, which contains the days for which we wish to make predictions. Subsequently, we generate training and evaluation datasets with **'TransformDataset'** instance.\n",
    "\n",
    "4. Early Stopping and Trainer Configuration:\n",
    "We integrate the early stopping callback to track the model's performance on the validation dataset. If the xxx metric doesn't improve for a defined number of epochs, the training process halts. We then configure the trainer settings, including the early stopping callback. The trainer is responsible for managing the training process.\n",
    "\n",
    "5. Model Training:\n",
    "We initiate the training of the DeepAR model using the training dataset train_ds. The model learns from the data to make predictions.\n",
    "\n",
    "6. Evaluation and Error Computation:\n",
    "To evaluate this model, we make predictions using the trained predictor for each date in 'date_list_daily'. Utilizing the make_evaluation_predictions method, we obtain forecasts and true time series (forecasts, tss). We calculate binary forecasts based on the median sample from the forecasts dataset, compare them with true values, and compute the daily error.\n",
    "\n",
    "7. Optuna Trial Update and Error Return:\n",
    "We update the Optuna trial's user attribute with the best trained model for future reference and return the cumulative daily_error, which signifies the total difference between the binary forecasts and the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b7a9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, trial):\n",
    "    params = self.get_params(trial)\n",
    "\n",
    "    transformer = Transform_Dataset(\n",
    "        self.start_training,\n",
    "        self.start_validation,\n",
    "        self.devices,\n",
    "        self.dataset_dict,\n",
    "        self.freq, \n",
    "        self.prediction_length)\n",
    "    \n",
    "    training_dataset = transformer.reshape_training_data()\n",
    "    \n",
    "    # Create static categorical features\n",
    "    cardinality = transformer.create_static_categorical_features(training_dataset)[1]\n",
    "\n",
    "    # Initialize the DeepAR estimator\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=self.prediction_length,\n",
    "        context_length=params['context_length'],\n",
    "        freq=self.freq,\n",
    "        distr_output=CategoricalOutput(2),\n",
    "        use_feat_dynamic_real=True,\n",
    "        use_feat_static_cat=True,\n",
    "        cardinality=cardinality,\n",
    "        num_layers=params['num_layers'],\n",
    "        num_cells=params['num_cells'],\n",
    "        batch_size=params['batch_size'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "    )\n",
    "    \n",
    "    evaluator = Evaluator(allow_nan_forecast=True)\n",
    "\n",
    "    train_ds, val_ds, _ = transformer.pipeline()\n",
    "    \n",
    "    es_callback = Metric_Inference_Early_Stopping(\n",
    "        validation_dataset=val_ds,\n",
    "        estimator=estimator,\n",
    "        metric='RMSE',\n",
    "        patience=20,\n",
    "        evaluator=evaluator\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        epochs=params['epochs'],\n",
    "        num_batches_per_epoch=params['num_batches_per_epoch'],\n",
    "        callbacks=[es_callback],\n",
    "        hybridize=False\n",
    "    )\n",
    "    \n",
    "    datelist_daily = pd.date_range(self.start_validation, freq='d', periods=self.recommendation_length)\n",
    "\n",
    "    estimator.trainer = trainer\n",
    "    global predictor\n",
    "    daily_error = 0\n",
    "    \n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    for date in datelist_daily:\n",
    "        \n",
    "        transformer = Transform_Dataset(\n",
    "            self.start_training,\n",
    "            date,\n",
    "            self.devices,\n",
    "            self.dataset_dict,\n",
    "            self.freq, \n",
    "            self.prediction_length)\n",
    "\n",
    "        _, _, test_ds = transformer.pipeline()\n",
    "\n",
    "        forecast_deepar, ts_deepar = make_evaluation_predictions(\n",
    "            dataset=test_ds,\n",
    "            predictor=predictor,\n",
    "            num_samples=100\n",
    "        )\n",
    "        forecasts = list(forecast_deepar)\n",
    "        tss = list(ts_deepar)\n",
    "\n",
    "        forecasts_bin = {}\n",
    "        for i in range(0, len(forecasts)):\n",
    "\n",
    "            forecasts_bin[i] = np.where(forecasts[i].quantile(0.5) >= 0.5, 1, 0)\n",
    "            daily_error += abs(np.where(forecasts_bin[i].sum() == 0, 0, 1) - np.where(tss[i][-24:].sum()[0] == 0, 0, 1))\n",
    "            \n",
    "    trial.set_user_attr(key='best_model', value=predictor)\n",
    "\n",
    "    return daily_error\n",
    "\n",
    "setattr(DeepAR_Tuning_Objective, '__call__', __call__)\n",
    "del __call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374be53",
   "metadata": {},
   "source": [
    "#### 4.2.4 Find the Best  Hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055754c",
   "metadata": {},
   "source": [
    "Finally we employ Optuna in the **'optimize_deepar_hyperparameters'** function. A study is initiated to minimize the objective function. Using the **'DeepAR_Tuning_Objective'** class, the objective encapsulates model training and evaluation. The optimization process searches for the best hyperparameters over a specified number of trials. and with the callback function, tracks the best trial, updating best_model_deepar when a better model is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0503f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_deepar_hyperparameters(\n",
    "    prediction_length, freq, start_dataset, start_validation, recommendation_length, devices, dataset_dict, n_trials=30, seed=0\n",
    "):\n",
    "    import optuna\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    best_model_deepar = None\n",
    "    predictor = None\n",
    "\n",
    "    def callback(study, trial):\n",
    "        nonlocal best_model_deepar\n",
    "        if study.best_trial == trial:\n",
    "            best_model_deepar = predictor\n",
    "\n",
    "    time_start = time.time()\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "    study.optimize(\n",
    "        DeepAR_Tuning_Objective(\n",
    "            prediction_length, freq, start_dataset, start_validation, recommendation_length, devices, dataset_dict\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "\n",
    "    print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial_p = study.best_trial\n",
    "\n",
    "    print('  Value: {}'.format(trial_p.value))\n",
    "\n",
    "    print('  Params: ')\n",
    "    for key, value in trial_p.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "    print('Elapsed time:', time.time() - time_start)\n",
    "\n",
    "    return best_model_deepar\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7f4a15d-75fd-41c4-a1a0-119d11657530",
   "metadata": {},
   "source": [
    "best_model = optimize_deepar_hyperparameters(\n",
    "    prediction_length,'H', start_dataset, start_validation, recommendation_length, devices, dataset_dict, n_trials=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673bb53",
   "metadata": {},
   "source": [
    "In the cell below, you can upload the pickle file for the best model. We will evaluate it on the next script. This DeepAR model has the following parameters:\n",
    "\n",
    "* Context length: 840\n",
    "* No of cells: 48\n",
    "* No of layers: 3\n",
    "* Learning rate: 0.01752\n",
    "* Dropout rate: 0.16056\n",
    "* Epocs: 31\n",
    "* Batch size: 128\n",
    "* No of batches per epoch: 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4dd400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Helper_Functions_Thesis.open_pickle_file(PICKLE_PATH,'deepar_model.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19edd1",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **Appendix A1: Complete Create_Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f289561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Create_Dataset:\n",
    "    '''\n",
    "    A class for constructing datasets for each device with all the necessary information to train\n",
    "    the forecasting model.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_dataset:\n",
    "        A string ('yyyy-mm-dd') to determine the start of the dataset.\n",
    "    end_dataset:\n",
    "        A string ('yyyy-mm-dd') to determine the end of the dataset.\n",
    "    devices:\n",
    "        A dictionary containing device-specific information.\n",
    "    load_dict:\n",
    "        A dictionary containing the output dataframes from the Load Agent for each household.\n",
    "    usage_dict:\n",
    "        A dictionary containing the output dataframes from the Usage Agent for each household.\n",
    "    activity_dict:\n",
    "        A dictionary containing the output dataframes from the Activity Agent for each household.\n",
    "    fill_na:\n",
    "        Bool, controls if missing values should be filled with zeros.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_dataset: str,\n",
    "        end_dataset: str,\n",
    "        devices: Dict[str, Any],\n",
    "        load_dict: Dict[str, pd.DataFrame],\n",
    "        usage_dict: Dict[str, pd.DataFrame],\n",
    "        activity_dict: Dict[str, pd.DataFrame],\n",
    "        fill_na = True,\n",
    "    ):\n",
    "        from helper_functions_thesis import Helper_Functions_Thesis\n",
    "        self.start_dataset = start_dataset\n",
    "        self.end_dataset = end_dataset\n",
    "        self.devices = devices\n",
    "        self.load_dict = load_dict\n",
    "        self.usage_dict = usage_dict\n",
    "        self.activity_dict = activity_dict\n",
    "        self.hh_list: List[str] = list(Helper_Functions_Thesis.create_shiftable_devices_dict(self.devices).keys())\n",
    "        self.fill_na = fill_na\n",
    "\n",
    "    def slice_datasets(self) -> None:\n",
    "        import pandas as pd\n",
    "\n",
    "        for hh in self.hh_list:\n",
    "            self.load_dict[hh] = self.load_dict[hh].loc[self.start_dataset:self.end_dataset, :]\n",
    "            self.usage_dict[hh] = self.usage_dict[hh].loc[self.start_dataset:self.end_dataset, :]\n",
    "            self.activity_dict[hh] = self.activity_dict[hh].loc[self.start_dataset:self.end_dataset, :]\n",
    "\n",
    "    def fill_na_function(self) -> None:\n",
    "        import pandas as pd\n",
    "\n",
    "        for hh in self.hh_list:\n",
    "            self.load_dict[hh] = self.load_dict[hh].fillna(0)\n",
    "            self.usage_dict[hh] = self.usage_dict[hh].fillna(0)\n",
    "            self.activity_dict[hh] = self.activity_dict[hh].fillna(0)\n",
    "\n",
    "    def create_activity_probability(self, activity_dict: pd.DataFrame) -> np.ndarray:\n",
    "        import numpy as np\n",
    "        from datetime import timedelta\n",
    "        from agents import Activity_Agent\n",
    "            \n",
    "        activity = Activity_Agent(activity_dict)\n",
    "        X_train, y_train, X_test, y_test = activity.train_test_split(\n",
    "            activity_dict, str(activity_dict.index[-1].date()+timedelta(1))\n",
    "        )\n",
    "        X_train = X_train.fillna(0)\n",
    "        model = activity.fit(X_train, y_train, 'random forest')\n",
    "        activity_probability = model.predict_proba(X_train)[:, 1]\n",
    "        return activity_probability\n",
    "\n",
    "    def create_usage_probability(self, usage_dict: pd.DataFrame, dev_name: str) -> np.ndarray:\n",
    "        import numpy as np\n",
    "        from agents import Usage_Agent\n",
    "        \n",
    "        usage = Usage_Agent(usage_dict, dev_name)\n",
    "        X_train, y_train, X_test, y_test = usage.train_test_split(\n",
    "            usage_dict, self.end_dataset, train_start=self.start_dataset\n",
    "        )\n",
    "        X_train = X_train.fillna(0) \n",
    "        model = usage.fit(X_train, y_train.values.ravel(), 'random forest')\n",
    "        usage_probability = model.predict_proba(X_train)[:, 1]\n",
    "        return usage_probability\n",
    "\n",
    "    def resample_daily_to_hourly_data(self, daily_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        import pandas as pd\n",
    "        from agents import Activity_Agent\n",
    "\n",
    "        dummy_day = daily_data.index[-1] + pd.Timedelta(days=1)\n",
    "        dummy_row = pd.DataFrame(index=pd.DatetimeIndex([dummy_day]))\n",
    "        daily_data_with_dummy = pd.concat([daily_data, dummy_row])\n",
    "        hourly_data = daily_data_with_dummy.resample(\"H\").ffill().iloc[:-1, :]\n",
    "        return hourly_data\n",
    "\n",
    "    def create_deepar_dataset(self) -> Dict[str, pd.DataFrame]:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        deepar_dataset = {}\n",
    "\n",
    "        for dev in self.devices.keys():\n",
    "            hh = self.devices[dev]['hh']\n",
    "            dev_name = self.devices[dev]['dev_name']\n",
    "\n",
    "            # Create a DataFrame with required columns\n",
    "            deepar_dataset[dev] = pd.DataFrame({\n",
    "                'temp': self.activity_dict[hh]['temp'],\n",
    "                'dwpt': self.activity_dict[hh]['dwpt'],\n",
    "                'rhum': self.activity_dict[hh]['rhum'],\n",
    "                'wdir': self.activity_dict[hh]['wdir'],\n",
    "                'wspd': self.activity_dict[hh]['wspd'],\n",
    "                'usage': self.load_dict[hh][dev_name],\n",
    "                'usage_bin': np.where(self.load_dict[hh][dev_name] > 0, 1, 0),\n",
    "                'hh': hh,\n",
    "                'dev': self.devices[dev]['dev'],\n",
    "                'activity_prob': self.create_activity_probability(self.activity_dict[hh])\n",
    "            })\n",
    "\n",
    "            # add usage-related features\n",
    "            columns = ['periods_since_last_activity', f'periods_since_last_{dev_name}_usage']\n",
    "            daily_features = self.usage_dict[hh][columns].copy()\n",
    "            daily_features.columns = daily_features.columns.str.replace(f'{dev_name}_', '', regex=False)\n",
    "            daily_features['usage_prob'] = self.create_usage_probability(self.usage_dict[hh].copy(), dev_name)\n",
    "\n",
    "            # Resample daily data to hourly and concatenate\n",
    "            hourly_features = self.resample_daily_to_hourly_data(daily_features)\n",
    "            deepar_dataset[dev] = pd.concat([deepar_dataset[dev], hourly_features], axis=1)\n",
    "\n",
    "            # Select final columns\n",
    "            columns = [\n",
    "                'usage', 'usage_bin', 'hh', 'dev',\n",
    "                'periods_since_last_activity', 'periods_since_last_usage',\n",
    "                'activity_prob', 'usage_prob',\n",
    "                'temp', 'dwpt', 'rhum', 'wdir', 'wspd'\n",
    "            ]\n",
    "\n",
    "            deepar_dataset[dev] = deepar_dataset[dev][columns]\n",
    "\n",
    "        return deepar_dataset\n",
    "\n",
    "\n",
    "\n",
    "    def pipeline(self) -> Dict[str, pd.DataFrame]:\n",
    "    \n",
    "        self.slice_datasets()\n",
    "        \n",
    "        if self.fill_na:\n",
    "                self.fill_na_function()\n",
    "\n",
    "        deepar_dataset = self.create_deepar_dataset()\n",
    "\n",
    "        return deepar_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148391c1",
   "metadata": {},
   "source": [
    "## **Appendix A2: Complete Transform_Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8cc6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Tuple\n",
    "import pandas as pd\n",
    "class Transform_Dataset:\n",
    "    '''\n",
    "    A class for reshaping datasets into a format compatible with GluonTS, the forecasting model.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_training:\n",
    "        A string ('yyyy-mm-dd') determining the start of the training period.\n",
    "    start_validation:\n",
    "        A string ('yyyy-mm-dd') determining the start of the testing period.\n",
    "    devices:\n",
    "        A dictionary containing device-specific information.\n",
    "    dataset_dict:\n",
    "        A dictionary with target time series data along with covariates.\n",
    "    freq:\n",
    "        A string specifying the frequency of the time series data (e.g., '1H' for hourly data).\n",
    "    prediction_length:\n",
    "        An integer representing the desired prediction horizon (default is 24).\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_training: str,\n",
    "        start_validation: str,\n",
    "        devices: Dict[str, Any],\n",
    "        dataset_dict: Dict[str, pd.DataFrame],\n",
    "        freq: str = '1H',\n",
    "        prediction_length: int = 24,\n",
    "    ):\n",
    "        self.start_training = str(start_training)\n",
    "        self.start_validation = str(start_validation)\n",
    "        self.hh_list: List[str] = []\n",
    "        self.devices = devices\n",
    "        self.dataset_dict = {hh: df.copy() for hh, df in dataset_dict.items()}\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def reshape_training_data(self):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        transposed_list = {}\n",
    "        \n",
    "        for dev in self.devices.keys():\n",
    "            \n",
    "            self.dataset_dict[dev] = self.dataset_dict[dev].loc[:self.start_validation]\n",
    "            usage_bin_df = self.dataset_dict[dev]['usage_bin']\n",
    "            usage_bin_df = usage_bin_df.reindex()\n",
    "            usage_bin_df = pd.DataFrame(usage_bin_df)\n",
    "            usage_bin_df.index = ['d' + str(n) for n in np.arange(usage_bin_df.shape[0])]\n",
    "            usage_bin_df = pd.DataFrame(usage_bin_df).T\n",
    "\n",
    "            usage_bin_df['dev'] = self.devices[dev]['dev']\n",
    "            usage_bin_df['hh'] = str(self.devices[dev]['hh'])\n",
    "            usage_bin_df['id'] = usage_bin_df['hh'] + '_' + usage_bin_df['dev']\n",
    "            transposed_list[dev] = usage_bin_df\n",
    "\n",
    "        training_dataset = pd.concat(transposed_list.values()).reset_index(drop=True)\n",
    "\n",
    "        return training_dataset\n",
    "\n",
    "    def create_dynamic_real_features(self):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        train_dynamic_real_features_list = []\n",
    "        val_dynamic_real_features_list = []\n",
    "        test_dynamic_real_features_list = []\n",
    "\n",
    "        for dev in self.devices.keys():\n",
    "            dynamic_real_columns = self.dataset_dict[dev].drop(columns=['usage','usage_bin','hh','dev']).columns\n",
    "\n",
    "            train_dynamic_real_features = (\n",
    "                self.dataset_dict[dev]\n",
    "                .iloc[: -self.prediction_length * 2, :][dynamic_real_columns]\n",
    "                .T.to_numpy()\n",
    "            )\n",
    "            val_dynamic_real_features = (\n",
    "                self.dataset_dict[dev]\n",
    "                .iloc[:-self.prediction_length, :][dynamic_real_columns]\n",
    "                .T.to_numpy()\n",
    "            )\n",
    "            test_dynamic_real_features = self.dataset_dict[dev][\n",
    "                dynamic_real_columns\n",
    "            ].T.to_numpy()\n",
    "\n",
    "            train_dynamic_real_features_list.append(train_dynamic_real_features)\n",
    "            val_dynamic_real_features_list.append(val_dynamic_real_features)\n",
    "            test_dynamic_real_features_list.append(test_dynamic_real_features)\n",
    "\n",
    "        return (\n",
    "            train_dynamic_real_features_list,\n",
    "            val_dynamic_real_features_list,\n",
    "            test_dynamic_real_features_list,\n",
    "        )\n",
    "\n",
    "    def create_static_categorical_features(self, training_dataset: pd.DataFrame) -> Tuple[Any, Tuple[int, int]]:\n",
    "        import numpy as np\n",
    "        \n",
    "        hh_ids = training_dataset['hh'].astype('category').cat.codes.values\n",
    "        hh_ids_unique = np.unique(hh_ids)\n",
    "\n",
    "        dev_ids = training_dataset['dev'].astype('category').cat.codes.values\n",
    "        dev_ids_unique = np.unique(dev_ids)\n",
    "\n",
    "        stat_cat_list = [hh_ids, dev_ids]\n",
    "\n",
    "        stat_cat = np.concatenate(stat_cat_list)\n",
    "        stat_cat = stat_cat.reshape(len(stat_cat_list), len(dev_ids)).T\n",
    "            \n",
    "        stat_cat_cardinalities: Tuple[int, int] = (len(hh_ids_unique), len(dev_ids_unique))\n",
    "            \n",
    "        return stat_cat, stat_cat_cardinalities\n",
    "\n",
    "    def split_data(\n",
    "    self,\n",
    "    training_dataset: pd.DataFrame,\n",
    "    train_dynamic_real_features_list: List[Any],\n",
    "    val_dynamic_real_features_list: List[Any],\n",
    "    test_dynamic_real_features_list: List[Any],\n",
    "    stat_cat: Any\n",
    ") -> Tuple[Any, Any, Any]:\n",
    "        import pandas as pd\n",
    "        from gluonts.dataset.common import ListDataset\n",
    "        from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "        train_df = training_dataset.drop(['id', 'hh', 'dev'], axis=1)\n",
    "        train_target_values = train_df.values\n",
    "\n",
    "        test_target_values = val_target_values = train_target_values.copy()\n",
    "\n",
    "        train_target_values = [ts[:-self.prediction_length * 2] for ts in train_target_values]\n",
    "        val_target_values = [ts[:-self.prediction_length] for ts in val_target_values]\n",
    "\n",
    "        dates = [pd.Timestamp(self.start_training) for _ in range(len(training_dataset))]\n",
    "\n",
    "        def create_list_dataset(target_values, dynamic_real_features_list):\n",
    "            return ListDataset(\n",
    "                [\n",
    "                    {\n",
    "                        FieldName.TARGET: target,\n",
    "                        FieldName.START: start,\n",
    "                        FieldName.FEAT_DYNAMIC_REAL: fdr,\n",
    "                        FieldName.FEAT_STATIC_CAT: fsc,\n",
    "                    }\n",
    "                    for (target, start, fdr, fsc) in zip(\n",
    "                        target_values,\n",
    "                        dates,\n",
    "                        dynamic_real_features_list,\n",
    "                        stat_cat,\n",
    "                    )\n",
    "                ],\n",
    "                freq=self.freq,\n",
    "            )\n",
    "\n",
    "        train_ds = create_list_dataset(train_target_values, train_dynamic_real_features_list)\n",
    "        val_ds = create_list_dataset(val_target_values, val_dynamic_real_features_list)\n",
    "        test_ds = create_list_dataset(test_target_values, test_dynamic_real_features_list)\n",
    "\n",
    "        return train_ds, val_ds, test_ds\n",
    "\n",
    "    def pipeline(self) -> Tuple[Any, Any, Any]:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from gluonts.dataset.common import ListDataset\n",
    "        from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "        training_dataset = self.reshape_training_data()\n",
    "\n",
    "        (\n",
    "            train_dynamic_real_features_list,\n",
    "            val_dynamic_real_features_list,\n",
    "            test_dynamic_real_features_list,\n",
    "        ) = self.create_dynamic_real_features()\n",
    "            \n",
    "        stat_cat = self.create_static_categorical_features(training_dataset)[0]\n",
    "\n",
    "        train_ds, val_ds, test_ds = self.split_data(\n",
    "            training_dataset,\n",
    "            train_dynamic_real_features_list,\n",
    "            val_dynamic_real_features_list,\n",
    "            test_dynamic_real_features_list,\n",
    "            stat_cat\n",
    "        )\n",
    "\n",
    "        return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9972f",
   "metadata": {},
   "source": [
    "## **Appendix A3: Complete Metric_Inference_Early_Stopping Class'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18443510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from gluonts.mx.trainer.callback import Callback\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.dataset.common import Dataset\n",
    "from gluonts.mx import DeepAREstimator\n",
    "class Metric_Inference_Early_Stopping(Callback):\n",
    "    '''\n",
    "    Early Stopping mechanism based on the prediction network.\n",
    "    Can be used to base the Early Stopping directly on a metric of interest, instead of on the training/validation loss.\n",
    "    In the same way as test datasets are used during model evaluation,\n",
    "    the time series of the validation_dataset can overlap with the train dataset time series,\n",
    "    except for a prediction_length part at the end of each time series.\n",
    "    Parameters\n",
    "    ----------\n",
    "    validation_dataset\n",
    "        An out-of-sample dataset which is used to monitor metrics\n",
    "    predictor\n",
    "        A gluon predictor, with a prediction network that matches the training network\n",
    "    evaluator\n",
    "        The Evaluator used to calculate the validation metrics.\n",
    "    metric\n",
    "        The metric on which to base the early stopping on.\n",
    "    patience\n",
    "        Number of epochs to train on given the metric did not improve more than min_delta.\n",
    "    min_delta\n",
    "        Minimum change in the monitored metric counting as an improvement\n",
    "    verbose\n",
    "        Controls, if the validation metric is printed after each epoch.\n",
    "    minimize_metric\n",
    "        The metric objective.\n",
    "    restore_best_network\n",
    "        Controls, if the best model, as assessed by the validation metrics is restored after training.\n",
    "    num_samples\n",
    "        The amount of samples drawn to calculate the inference metrics.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        validation_dataset: Dataset,\n",
    "        estimator: DeepAREstimator,\n",
    "        evaluator: Evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9],allow_nan_forecast=True),\n",
    "        metric: str = 'MSE',\n",
    "        patience: int = 20,\n",
    "        min_delta: float = 0.0,\n",
    "        verbose: bool = False,\n",
    "        minimize_metric: bool = True,\n",
    "        restore_best_network: bool = True,\n",
    "        num_samples: int = 100,\n",
    "    ):\n",
    "        assert (\n",
    "            patience >= 0\n",
    "        ), 'EarlyStopping Callback patience needs to be >= 0'\n",
    "        assert (\n",
    "            min_delta >= 0\n",
    "        ), 'EarlyStopping Callback min_delta needs to be >= 0.0'\n",
    "        assert (\n",
    "            num_samples >= 1\n",
    "        ), 'EarlyStopping Callback num_samples needs to be >= 1'\n",
    "\n",
    "        self.validation_dataset = list(validation_dataset)\n",
    "        self.estimator = estimator\n",
    "        self.evaluator = evaluator\n",
    "        self.metric = metric\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.restore_best_network = restore_best_network\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        if minimize_metric:\n",
    "            self.best_metric_value = np.inf\n",
    "            self.is_better = np.less\n",
    "        else:\n",
    "            self.best_metric_value = -np.inf\n",
    "            self.is_better = np.greater\n",
    "\n",
    "        self.validation_metric_history: List[float] = []\n",
    "        self.best_network = None\n",
    "        self.n_stale_epochs = 0\n",
    "\n",
    "    def on_epoch_end(\n",
    "        self,\n",
    "        epoch_no: int,\n",
    "        epoch_loss: float,\n",
    "        training_network: mx.gluon.nn.HybridBlock,\n",
    "        trainer: mx.gluon.Trainer,\n",
    "        best_epoch_info: dict,\n",
    "        ctx: mx.Context\n",
    "    ) -> bool:\n",
    "        should_continue = True\n",
    "        \n",
    "        transformation = self.estimator.create_transformation()\n",
    "        predictor = self.estimator.create_predictor(transformation=transformation, trained_network=training_network)\n",
    "\n",
    "        from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=self.validation_dataset,\n",
    "            predictor=predictor,\n",
    "            num_samples=self.num_samples,\n",
    "        )\n",
    "\n",
    "        agg_metrics, item_metrics = self.evaluator(ts_it, forecast_it)\n",
    "        current_metric_value = agg_metrics[self.metric]\n",
    "        self.validation_metric_history.append(current_metric_value)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation metric {self.metric}: {current_metric_value}, best: {self.best_metric_value}'\n",
    "            )\n",
    "\n",
    "        if self.is_better(current_metric_value, self.best_metric_value):\n",
    "            self.best_metric_value = current_metric_value\n",
    "\n",
    "            if self.restore_best_network:\n",
    "                training_network.save_parameters('best_network.params')\n",
    "\n",
    "            self.n_stale_epochs = 0\n",
    "        else:\n",
    "            self.n_stale_epochs += 1\n",
    "            if self.n_stale_epochs == self.patience:\n",
    "                should_continue = False\n",
    "                print(\n",
    "                    f'EarlyStopping callback initiated stop of training at epoch {epoch_no}.'\n",
    "                )\n",
    "\n",
    "                if self.restore_best_network:\n",
    "                    print(\n",
    "                        f'Restoring best network from epoch {epoch_no - self.patience}.'\n",
    "                    )\n",
    "                    training_network.load_parameters('best_network.params')\n",
    "\n",
    "        return should_continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9f302",
   "metadata": {},
   "source": [
    "## **Appendix A4: Complete DeepAR_Tuning_Objective Class'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b1146ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepAR_Tuning_Objective:\n",
    "    '''\n",
    "    A class for hyperparameter tuning using Optuna.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prediction_length:\n",
    "        An integer representing the desired prediction horizon.\n",
    "    freq:\n",
    "        A string specifying the frequency of the time series data.\n",
    "    start_training:\n",
    "        A string ('yyyy-mm-dd') determining the start of the training period.\n",
    "    start_validation:\n",
    "        A string ('yyyy-mm-dd') determining the start of the validation period.\n",
    "    recommendation_length:\n",
    "        The length of the test period in days.\n",
    "    devices:\n",
    "        A dictionary containing device-specific information.\n",
    "    dataset_dict:\n",
    "        A dictionary with target time series data along with covariates.\n",
    "    metric_type:\n",
    "        A string specifying the type of metric to optimize during tuning (default is 'MASE').\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length: int,\n",
    "        freq: str,\n",
    "        start_training: str,\n",
    "        start_validation: str,\n",
    "        recommendation_length: int,\n",
    "        devices: Dict[str, Any],\n",
    "        dataset_dict: Dict[str, pd.DataFrame],\n",
    "        metric_type='MASE', \n",
    "    ):\n",
    "        self.prediction_length = prediction_length\n",
    "        self.freq = freq\n",
    "        self.start_training = start_training\n",
    "        self.start_validation = start_validation\n",
    "        self.recommendation_length = recommendation_length\n",
    "        self.devices = devices\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.metric_type = metric_type\n",
    "        \n",
    "    def get_params(self, trial) -> dict:\n",
    "        return {\n",
    "            # Number of time steps in the context window (3, 4, or 5 weeks)\n",
    "            'context_length': trial.suggest_categorical('context_length', [504, 672, 840]),\n",
    "            \n",
    "            # Number of layers in the network\n",
    "            'num_layers': trial.suggest_int('num_layers', 3, 4),\n",
    "            \n",
    "            # Number of cells in each layer\n",
    "            'num_cells': trial.suggest_int('num_cells', 10, 50),\n",
    "            \n",
    "            # Dropout rate to prevent overfitting\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "            \n",
    "            # Learning rate for optimization (log scale)\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "            \n",
    "            # Number of training epochs\n",
    "            'epochs': trial.suggest_int('epochs', 25, 100),\n",
    "            \n",
    "            # Batch size for training\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "            \n",
    "            # Number of batches per epoch\n",
    "            'num_batches_per_epoch': trial.suggest_categorical('num_batches_per_epoch', [40, 50, 60, 70, 80, 90]),\n",
    "        }\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        params = self.get_params(trial)\n",
    "\n",
    "        transformer = Transform_Dataset(\n",
    "            self.start_training,\n",
    "            self.start_validation,\n",
    "            self.devices,\n",
    "            self.dataset_dict,\n",
    "            self.freq, \n",
    "            self.prediction_length)\n",
    "        \n",
    "        training_dataset = transformer.reshape_training_data()\n",
    "        \n",
    "        # Create static categorical features\n",
    "        cardinality = transformer.create_static_categorical_features(training_dataset)[1]\n",
    "\n",
    "        # Initialize the DeepAR estimator\n",
    "        estimator = DeepAREstimator(\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=params['context_length'],\n",
    "            freq=self.freq,\n",
    "            distr_output=CategoricalOutput(2),\n",
    "            use_feat_dynamic_real=True,\n",
    "            use_feat_static_cat=True,\n",
    "            cardinality=cardinality,\n",
    "            num_layers=params['num_layers'],\n",
    "            num_cells=params['num_cells'],\n",
    "            batch_size=params['batch_size'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "        )\n",
    "        \n",
    "        evaluator = Evaluator(allow_nan_forecast=True)\n",
    "\n",
    "        train_ds, val_ds, _ = transformer.pipeline()\n",
    "        \n",
    "        es_callback = Metric_Inference_Early_Stopping(\n",
    "            validation_dataset=val_ds,\n",
    "            estimator=estimator,\n",
    "            metric='RMSE',\n",
    "            patience=20,\n",
    "            evaluator=evaluator\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            learning_rate=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            num_batches_per_epoch=params['num_batches_per_epoch'],\n",
    "            callbacks=[es_callback],\n",
    "            hybridize=False\n",
    "        )\n",
    "        \n",
    "        datelist_daily = pd.date_range(self.start_validation, freq='d', periods=self.recommendation_length)\n",
    "\n",
    "        estimator.trainer = trainer\n",
    "        global predictor\n",
    "        daily_error = 0\n",
    "        \n",
    "\n",
    "        predictor = estimator.train(train_ds)\n",
    "\n",
    "        for date in datelist_daily:\n",
    "            \n",
    "            transformer = Transform_Dataset(\n",
    "                self.start_training,\n",
    "                date,\n",
    "                self.devices,\n",
    "                self.dataset_dict,\n",
    "                self.freq, \n",
    "                self.prediction_length)\n",
    "\n",
    "            _, _, test_ds = transformer.pipeline()\n",
    "\n",
    "            forecast_deepar, ts_deepar = make_evaluation_predictions(\n",
    "                dataset=test_ds,\n",
    "                predictor=predictor,\n",
    "                num_samples=100\n",
    "            )\n",
    "            forecasts = list(forecast_deepar)\n",
    "            tss = list(ts_deepar)\n",
    "\n",
    "            forecasts_bin = {}\n",
    "            for i in range(0, len(forecasts)):\n",
    "\n",
    "                forecasts_bin[i] = np.where(forecasts[i].quantile(0.5) >= 0.5, 1, 0)\n",
    "                daily_error += abs(np.where(forecasts_bin[i].sum() == 0, 0, 1) - np.where(tss[i][-24:].sum()[0] == 0, 0, 1))\n",
    "                \n",
    "        trial.set_user_attr(key='best_model', value=predictor)\n",
    "\n",
    "        return daily_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f54c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
